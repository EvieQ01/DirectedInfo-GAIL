\documentclass{article} % For LaTeX2e
\usepackage{iclr2019_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% My packages
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{amsmath, amsfonts, bm}
\usepackage{amssymb}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{color}
\usepackage{booktabs}

\title{Supplementary material for Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.  Funding acknowledgements go at the end of the paper.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213, USA \\
\texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
\And
Ji Q. Ren \& Yevgeny LeNet \\
Department of Computational Neuroscience \\
University of the Witwatersrand \\
Joburg, South Africa \\
\texttt{\{robot,net\}@wits.ac.za} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\begin{document}
\maketitle

%===============================================================================
\section{Derivation for Directed-Info Loss}
The directed information flow from a sequence $\boldsymbol{X}$ to $\boldsymbol{Y}$ is given by,

\begin{equation*}
I(\boldsymbol{X} \rightarrow \boldsymbol{Y}) = H(\boldsymbol{Y}) - H(\boldsymbol{Y} \| \boldsymbol{X})    
\end{equation*}

where $H(\boldsymbol{Y} \| \boldsymbol{X})$ is the causally-conditioned entropy. Replacing $\boldsymbol{X}$ and $\boldsymbol{Y}$ with the sequences $\boldsymbol{\tau}$ and $\boldsymbol{c}$ gives,

\begin{align}
\footnotesize
I(\boldsymbol{\tau} \rightarrow \boldsymbol{c}) &= H(\boldsymbol{c}) - H(\boldsymbol{c} \| \boldsymbol{\tau})& &\nonumber \\
&= H(\boldsymbol{c}) - \sum_{t}H(c^{t}|c^{1:t-1}, \tau^{1:t})& & \nonumber \\
&= H(\boldsymbol{c}) + \sum_{t} \sum_{c^{1:t-1}, \tau^{1:t}} \Big[ p(c^{1:t-1}, \tau^{1:t}) & &\nonumber \\
& \sum_{c^{t}} p(c^{t} | c^{1:t-1}, \tau^{1:t}) \log p(c^{t} | c^{1:t-1}, \tau^{1:t})\Big]  \nonumber \\
&= H(\boldsymbol{c}) + \sum_{t} \sum_{c^{1:t-1}, \tau^{1:t}} \Big[ p(c^{1:t-1}, \tau^{1:t}) & & \nonumber\\
& [D_{KL}(p(\cdot | c^{1:t-1}, \tau^{1:t}) \| q(\cdot | c^{1:t-1}, \tau^{1:t})) \nonumber \\
& \hspace{-5pt}+ \sum_{c^{t}} p(c^{t} | c^{1:t-1}, \tau^{1:t}) \log q(c^{t} | c^{1:t-1}, \tau^{1:t})]\Big]   \nonumber \\
&\geq H(\boldsymbol{c}) + \sum_{t} \sum_{c^{1:t-1}, \tau^{1:t}} \Big[ p(c^{1:t-1}, \tau^{1:t}) & & \nonumber\\
& \sum_{c^{t}} p(c^{t} | c^{1:t-1}, \tau^{1:t}) \log q(c^{t} | c^{1:t-1}, \tau^{1:t})\Big]
\label{eq:directed_info}
\end{align}

Here $\tau^{1:t} = (s_1, \cdots, a_{t-1}, s_{t})$. The lower bound in equation~\ref{eq:directed_info} requires us to know the true posterior distribution to compute the expectation. To avoid sampling from $p(c^{t}|c^{1:t-1}, \tau^{1:t})$, we use the following,

\begin{align}
 \sum_{c^{1:t-1}, \tau^{1:t}}& \Big[ p(c^{1:t-1}, \tau^{1:t}) & \nonumber\\
& \sum_{c^{t}} p(c^{t} | c^{1:t-1}, \tau^{1:t}) \nonumber\\
& \log q(c^{t} | c^{1:t-1}, \tau^{1:t})\Big] \nonumber \\
= \sum_{c^{1:t-1}, \tau^{1:t},c^{t}}& \Big[ p(c^{1:t-1}, \tau^{1:t}) p(c^{t} | c^{1:t-1}, \tau^{1:t}) & &\nonumber\\
& \log q(c^{t} | c^{1:t-1}, \tau^{1:t})\Big] \nonumber \\
= \sum_{c^{1:t-1}, \tau^{1:t},c^{t}}& \Big[ p(c^{t}, c^{1:t-1}, \tau^{1:t}) \nonumber\\
& \log q(c^{t} | c^{1:t-1}, \tau^{1:t})] \nonumber \\
= \sum_{c^{1:t-1}, \tau^{1:t},c^{t}}&  \Big[p(\tau^{1:t} | c^{t}, c^{1:t-1}) p(c^{t}, c^{1:t-1}) \nonumber\\
& \log q(c^{t} | c^{1:t-1}, \tau^{1:t}) \Big] \nonumber \\
% \end{align}
% \begin{align}
= \sum_{c^{1:t}} p(c^{1:t})\sum_{\tau^{1:t}}&  \Big[p(\tau^{1:t} | c^{t}, c^{1:t-1}) \nonumber\\
& \log q(c^{t} | c^{1:t-1}, \tau^{1:t}) \Big] \nonumber \\
= \sum_{c^{1:t}} p(c^{1:t})\sum_{\tau^{1:t}}&  \Big[p(\tau^{1:t} | c^{1:t-1}) \nonumber\\
& \log q(c^{t} | c^{1:t-1}, \tau^{1:t}) \Big]
\label{eq:directed_info2}
\end{align}

where the last step follows from the causal restriction that future provided variables ($c^t$) do not  influence earlier predicted variables ($\tau^{1:t}$ consists of states upto time $t$. $c_t$ does not effect state $s_t$). Putting the result in equation~\ref{eq:directed_info2} in equation~\ref{eq:directed_info} gives,

\begin{align}
\begin{split}
 L_{1}(\pi, q) &= \sum_{t} \mathbb{E}_{c^{1:t} \sim p(c^{1:t}), a^{t-1} \sim \pi(\cdot | s^{t-1},c^{1:t-1})} \Big[ \\
 & \log q(c^{t} | c^{1:t-1}, \tau^{1:t}) \Big] + H(\boldsymbol{c}) \leq I(\boldsymbol{\tau} \rightarrow \boldsymbol{c})
\label{eq:loss1}
\end{split}
\end{align}

Thus, by maximizing directed information instead of mutual information, we can learn a posterior distribution over the next latent factor $c$ given the latent factors discovered up to now and the trajectory followed up to now, thereby removing the dependence on the future trajectory. In practice, we do not consider the $H(\boldsymbol{c})$ term. This gives us the objective,

\begin{align}
    \min_{\pi, q} \max_{D} & ~~\mathbb{E}_{\pi} [\log D(s,a)] + \mathbb{E}_{\pi_E} [1-\log D(s,a)] \nonumber\\
    & -\lambda_{1}L_{1}(\pi, q) - \lambda_{2}H(\pi)
\label{eq:directed_info_loss}
\end{align}

In practice, we fix $q$ from the VAE pre-training and only minimize over the policy $\pi$ in~\eqref{eq:directed_info_loss}.
% no \bibliographystyle is required, since the corl style is automatically used.
%\bibliography{example}  % .bib

\section{Implementation Details}
We use multi-layer perceptrons for our policy (Generator), value, and reward (Discriminator) function representations. Each network consisted of 2 hidden layers with 64 units in each layer and `tanh` as our non-linearity function. We used the Adam ~\cite{kingma2014adam} as our optimizer setting an initial learning rate of $3e^{-4}$. 
Further, we used the Proximal Policy Optimization algorithm ~\cite{schulman2017proximal} to train our policy network with $\epsilon = 0.2$


\end{document}